---
title: 'Notes: Examples of LLMs as Synthetic Human Responders and Agents'
date: 2025-10-29
permalink: /posts/2025/2025-10-29-notes-examples-of-llms-as-synthetic-human-responders-and-agents.md
tags:
- projects
- LLMs
- AI
- research
- notebooks
---
This post discusses the use of large language models (LLMs) as synthetic human responders and agents in various research contexts. It includes examples of LLMs simulating human decision-making, generating personas, and their application in agent-based models.

_Note: This is output from ChatGPT5 Plus’s Deep Research (light) mode, 25 SEP 2025. References have been checked and some light editing done by me – Janne M. Korhonen._ _I’m going to return to this topic later, but due to popular demand, chose to share the note as-is._

Researchers have begun using large language models (LLMs) to **simulate human decision-making** in surveys, experiments, and agent-based models. For example, Stanford HAI built _“digital twin”_ agents of 1,052 people by feeding two-hour interview transcripts into an LLM. These AI agents answered survey questions and played economic games in ways that _“mirror”_ their real-life counterparts [[1]](https://hai.stanford.edu/assets/files/hai-policy-brief-simulating-human-behavior-with-ai-agents.pdf#:~:text=against%20the%20corresponding%20person%E2%80%99s%20responses,on%20the%20General%20Social%20Survey)[[2]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=Interview,certain%20types%20of%20behavior%20simulations). In fact, the synthetic agents’ survey responses were **85% as accurate** as the real participants’ own answers on a follow-up General Social Survey (GSS) [[1]](https://hai.stanford.edu/assets/files/hai-policy-brief-simulating-human-behavior-with-ai-agents.pdf#:~:text=against%20the%20corresponding%20person%E2%80%99s%20responses,on%20the%20General%20Social%20Survey). Similarly, researchers Kim and Lee (2024) finetuned an LLM on decades of GSS data; their model could _fill in missing responses_ and _backfill_ longitudinal surveys with about 78% accuracy [[3]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=Kim%20and%20Lee%20found%20that,or%20for%20completing%20longitudinal%20datasets). These successes indicate that LLMs can act as proxy respondents: one study describes them as **“silicon samples”** that complement “human samples” by generating realistic answers and decision patterns when given persona information [[4]](https://arxiv.org/html/2503.16527v1#:~:text=simulations%20is%20the%20use%20of,3)[[5]](https://arxiv.org/html/2503.16527v1#:~:text=Meanwhile%2C%20LLM%20itself%20presents%20a,they%20faithfully%20capture%20a%20specific).

Researchers have started benchmarking LLMs explicitly as survey simulators. Liu et al. (2025) introduce **LLM-S³**, a comprehensive evaluation suite for _“virtual survey respondents.”_ They propose two modes: (1) **Partial Attribute Simulation (PAS)**, where the LLM infers missing demographics or answers from a partial profile, and (2) **Full Attribute Simulation (FAS)**, where the LLM generates entire synthetic survey datasets. Tested on 11 real-world datasets (politics, economics, health, etc.), LLMs showed _consistent but imperfect_ performance. Contextual prompts and few-shot examples significantly improve fidelity, but generating fully coherent structured answers remains challenging [[6]](https://arxiv.org/html/2509.06337v1#:~:text=simulating%20virtual%20survey%20respondents%20using,3.5%2F4%20Turbo%2C%20LLaMA)[[7]](https://arxiv.org/html/2509.06337v1#:~:text=%28GPT,and%20deployment%20in%20this%20emerging). Another recent preprint (“LLM-Mirror”) tested LLMs at individual-level replication: giving each respondent’s demographics and past answers as a prompt, the LLM generated the rest of their survey responses. This method (sometimes augmented with a synthetic **user persona** prompt) produced a set of answers whose statistical relationships matched the original data. In their experiment, structural-equation estimates from the LLM-generated survey were nearly identical to those from the real survey [[8]](https://arxiv.org/pdf/2412.03162#:~:text=%E2%80%A2%20By%20incorporating%20survey%20respondents%E2%80%99,have%20profound%20implications%20for%20augmenting). These studies show LLMs can reliably reproduce broad patterns of human responses and even individual answers when properly conditioned.

Industry and design-research practitioners are also using LLMs as surrogate users. In marketing research, the “study boosting” approach transforms a completed survey’s respondents into AI personas. AskRally (2025) describes converting real survey segments into “living AI personas” by copying each segment’s answers into the LLM’s memory[[9]](https://askrally.com/article/study-boosting-with-ai-personas#:~:text=Researchers%20often%20face%20the%20limitation,grounded%20voices)[[10]](https://askrally.com/article/study-boosting-with-ai-personas#:~:text=Each%20persona%20received%20comprehensive%20demographic,rather%20than%20generic%20demographic%20assumptions). These personas carry the actual survey respondents’ voice and preferences so teams can test new scenarios or questions without new data collection. Likewise, tools are emerging to automate LLM-based surveys: for example, an **EDSL** Python library lets users design experiments and “simulate responses with LLMs” as if they were survey participants [[11]](https://medium.com/thoughts-on-machine-learning/create-and-analyze-llm-based-surveys-using-edsl-1-f50ede7ecaf0#:~:text=The%20Emeritus%20Domain,understanding%20human%20behavior%20and%20preferences)[[12]](https://medium.com/thoughts-on-machine-learning/create-and-analyze-llm-based-surveys-using-edsl-1-f50ede7ecaf0#:~:text=enables%20users%20to%20design%20surveys,dimension%20to%20gathering%20insights%20and). In UX research, Nielsen Norman’s recent review highlights three studies using LLM-driven “digital twins” and “synthetic users.” They report that LLM-based twins (especially when built from rich interview transcripts) can accurately infer missing survey answers and predict behaviors across individual and group levels [[13]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=Digital%20twins%20and%20synthetic%20users,in%20the%20three%20studies%20reviewed)[[2]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=Interview,certain%20types%20of%20behavior%20simulations). By contrast, simpler synthetic users built only from demographics capture only general trends and tend to underestimate variability in opinions [[13]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=Digital%20twins%20and%20synthetic%20users,in%20the%20three%20studies%20reviewed)[[14]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=,behaviors%20better%20than%20synthetic%20users).

## LLM-Generated Personas and Opinion Simulation

Another major use of LLMs is to **generate persona profiles** or simulate public opinions. Large-scale studies have shown that LLMs can produce synthetic personas that embody demographic and psychographic traits. Ang _et al._ (2025) note that LLM-generated personas have already been used to conduct surveys, market research, and even “societal-scale simulations” [[5]](https://arxiv.org/html/2503.16527v1#:~:text=Meanwhile%2C%20LLM%20itself%20presents%20a,they%20faithfully%20capture%20a%20specific). In their work they generated ~1,000,000 personas and found biases (e.g. a skew toward one political party) that highlight the need for care. In general, when an LLM prompt includes a persona description or user attributes, the model tends to “role-play” that persona, producing responses consistent with the given profile [[5]](https://arxiv.org/html/2503.16527v1#:~:text=Meanwhile%2C%20LLM%20itself%20presents%20a,they%20faithfully%20capture%20a%20specific)[[4]](https://arxiv.org/html/2503.16527v1#:~:text=simulations%20is%20the%20use%20of,3). For example, prompting an LLM with a synthetic persona allowed it to predict a person’s survey answers much more accurately than without any persona [[8]](https://arxiv.org/pdf/2412.03162#:~:text=%E2%80%A2%20By%20incorporating%20survey%20respondents%E2%80%99,have%20profound%20implications%20for%20augmenting)[[4]](https://arxiv.org/html/2503.16527v1#:~:text=simulations%20is%20the%20use%20of,3).

Studies of “persona prompting” show mixed results. Hu & Collier (2024) found that explicitly adding annotated persona features (age, gender, values, etc.) to LLM prompts can _modestly_ improve its ability to mimic diverse opinions. In their experiments, powerful models (GPT-4-scale) captured ~81% of the variance of human annotators when persona info was given [[15]](https://arxiv.org/html/2402.10811v2#:~:text=Large%20language%20models%20,our%20setting%3A%20the%20stronger%20the)[[16]](https://arxiv.org/html/2402.10811v2#:~:text=correlation%20between%20persona%20variables%20and,1%7D1%20Code). However, they also report that in many tasks those socio-demographic variables explain less than 10% of variation in responses [[15]](https://arxiv.org/html/2402.10811v2#:~:text=Large%20language%20models%20,our%20setting%3A%20the%20stronger%20the). In practice this means LLM personas can simulate broad stance differences, but may not capture finer-grained individual variation.

Interactive design tools leverage LLM personas for creative purposes. _PersonaFlow_ (Liu et al. 2024) used multiple LLM-simulated expert personas to critique research ideas. In a user study, participants reported that **multiple AI personas** gave more relevant and creative feedback than a single voice[[17]](https://arxiv.org/html/2409.12538v1#:~:text=of%20utilizing%20LLM,reliance%20and). Similarly, recent work on Human–AI workflows in persona generation shows that LLMs can summarize clustered user data into believable persona profiles, and that allowing human researchers to guide the grouping leads to more representative personas [[18]](https://joongishin.github.io/perGenWorkflow/material/persona-generation-workflow.pdf#:~:text=bring%20to%20the%20generation%20of,We%20found). These examples demonstrate how simulated users can augment design research: an LLM persona can act like a focus-group participant or domain expert.

## Agent-Based and Decision-Model Simulation

LLM-driven agents have also been integrated into **agent-based models (ABM)** and game simulations. Gao _et al._ (2023) survey this area, noting that LLM agents can “adaptively react” and plan actions like humans, enabling richer simulations than simple rule-based agents [[19]](https://arxiv.org/html/2312.11970v1#:~:text=First%2C%20the%20LLM%20agent%20can,three%20strengths%2C%20LLM%20agents%20have). For instance, we have seen LLM agents engaging in social games: a NeurIPS (2024) paper found that GPT-4 agents playing iterative Trust Games behaved very similarly to humans. Specifically, these AI agents “manifest high behavioral alignment” with human trust decisions [[20]](https://proceedings.neurips.cc/paper_files/paper/2024/file/1cb57fcf7ff3f6d37eebae5becc9ea6d-Paper-Conference.pdf#:~:text=have%20our%20second%20core%20finding%3A,human%20interactions%20and%20societal%20institutions). That work concludes that LLMs can feasibly model fundamental human behaviors (like trusting others) within economic and social settings.

In summary, **academic and industry examples abound** of using LLMs to simulate people. They include formal studies (e.g. survey replication, persona generation experiments, game-theoretic simulations) and practical tools (surveys with AI respondents, persona-generators, UX “digital twin” systems). Collectively, these efforts show that LLMs can generate synthetic survey answers, embody user personas, and interact with decision models as stand-ins for humans [[1]](https://hai.stanford.edu/assets/files/hai-policy-brief-simulating-human-behavior-with-ai-agents.pdf#:~:text=against%20the%20corresponding%20person%E2%80%99s%20responses,on%20the%20General%20Social%20Survey)[[5]](https://arxiv.org/html/2503.16527v1#:~:text=Meanwhile%2C%20LLM%20itself%20presents%20a,they%20faithfully%20capture%20a%20specific). However, researchers also warn that without careful calibration this can introduce biases [[5]](https://arxiv.org/html/2503.16527v1#:~:text=Meanwhile%2C%20LLM%20itself%20presents%20a,they%20faithfully%20capture%20a%20specific)[[21]](https://arxiv.org/html/2503.16527v1#:~:text=potential%20of%20LLM%20persona%20simulation,stereotypes%2C%20and%20potential%20harm%20to). As this area matures, best practices (combining real data with AI prompts, extensive validation, etc.) are being developed to ensure reliability.

## References

[[1]](https://hai.stanford.edu/assets/files/hai-policy-brief-simulating-human-behavior-with-ai-agents.pdf#:~:text=against%20the%20corresponding%20person%E2%80%99s%20responses,on%20the%20General%20Social%20Survey) Park, J. S., Zou, C. Q., Shaw, A., Hill, B. M., Cai, C. J., Morris, M. R., Willer, R., Liang, P., & Bernstein, M. S. (n.d.). _Simulating Human Behavior with AI Agents_ [Policy brief]. Retrieved 25 September 2025, from [https://hai.stanford.edu/assets/files/hai-policy-brief-simulating-human-behavior-with-ai-agents.pdf](https://hai.stanford.edu/assets/files/hai-policy-brief-simulating-human-behavior-with-ai-agents.pdf)

[[2]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=Interview,certain%20types%20of%20behavior%20simulations) [[3]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=Kim%20and%20Lee%20found%20that,or%20for%20completing%20longitudinal%20datasets) [[13]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=Digital%20twins%20and%20synthetic%20users,in%20the%20three%20studies%20reviewed) [[14]](https://www.nngroup.com/articles/ai-simulations-studies/#:~:text=,behaviors%20better%20than%20synthetic%20users) Evaluating AI-Simulated Behavior: Insights from Three Studies on Digital Twins and Synthetic Users – NN/G [https://www.nngroup.com/articles/ai-simulations-studies/](https://www.nngroup.com/articles/ai-simulations-studies/)

[[4]](https://arxiv.org/html/2503.16527v1#:~:text=simulations%20is%20the%20use%20of,3) [[5]](https://arxiv.org/html/2503.16527v1#:~:text=Meanwhile%2C%20LLM%20itself%20presents%20a,they%20faithfully%20capture%20a%20specific) [[21]](https://arxiv.org/html/2503.16527v1#:~:text=potential%20of%20LLM%20persona%20simulation,stereotypes%2C%20and%20potential%20harm%20to) Li, A., Chen, H., Namkoong, H., & Peng, T. (2025). _LLM Generated Persona is a Promise with a Catch_ (No. arXiv:2503.16527). arXiv. [https://doi.org/10.48550/arXiv.2503.16527](https://doi.org/10.48550/arXiv.2503.16527)

[[6]](https://arxiv.org/html/2509.06337v1#:~:text=simulating%20virtual%20survey%20respondents%20using,3.5%2F4%20Turbo%2C%20LLaMA) [[7]](https://arxiv.org/html/2509.06337v1#:~:text=%28GPT,and%20deployment%20in%20this%20emerging) Zhao, J., Yuan, C., Luo, W., Xie, H., Zhang, G., Quan, S. J., Yuan, Z., Wang, P., & Zhang, D. (2025). _Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation_ (No. arXiv:2509.06337). arXiv. [https://doi.org/10.48550/arXiv.2509.06337](https://doi.org/10.48550/arXiv.2509.06337)

[[8]](https://arxiv.org/pdf/2412.03162#:~:text=%E2%80%A2%20By%20incorporating%20survey%20respondents%E2%80%99,have%20profound%20implications%20for%20augmenting) Kim, S., Jeong, J., Han, J. S., & Shin, D. (2024). _LLM-Mirror: A Generated-Persona Approach for Survey Pre-Testing_ (No. arXiv:2412.03162). arXiv. [https://doi.org/10.48550/arXiv.2412.03162](https://doi.org/10.48550/arXiv.2412.03162)

[[9]](https://askrally.com/article/study-boosting-with-ai-personas#:~:text=Researchers%20often%20face%20the%20limitation,grounded%20voices) [[10]](https://askrally.com/article/study-boosting-with-ai-personas#:~:text=Each%20persona%20received%20comprehensive%20demographic,rather%20than%20generic%20demographic%20assumptions) Study Boosting: Using Real Survey Data To Create Authentic AI Personas for Extended Research | Ask Rally [https://askrally.com/article/study-boosting-with-ai-personas](https://askrally.com/article/study-boosting-with-ai-personas)

[[11]](https://medium.com/thoughts-on-machine-learning/create-and-analyze-llm-based-surveys-using-edsl-1-f50ede7ecaf0#:~:text=The%20Emeritus%20Domain,understanding%20human%20behavior%20and%20preferences) [[12]](https://medium.com/thoughts-on-machine-learning/create-and-analyze-llm-based-surveys-using-edsl-1-f50ede7ecaf0#:~:text=enables%20users%20to%20design%20surveys,dimension%20to%20gathering%20insights%20and) Create and analyze LLM-based surveys using EDSL (1) | by FS Ndzomga [https://medium.com/thoughts-on-machine-learning/create-and-analyze-llm-based-surveys-using-edsl-1-f50ede7ecaf0](https://medium.com/thoughts-on-machine-learning/create-and-analyze-llm-based-surveys-using-edsl-1-f50ede7ecaf0)

[[15]](https://arxiv.org/html/2402.10811v2#:~:text=Large%20language%20models%20,our%20setting%3A%20the%20stronger%20the) [[16]](https://arxiv.org/html/2402.10811v2#:~:text=correlation%20between%20persona%20variables%20and,1%7D1%20Code) Hu, T., & Collier, N. (2024). Quantifying the Persona Effect in LLM Simulations (No. arXiv:2402.10811). [https://arxiv.org/html/2402.10811v2](https://arxiv.org/html/2402.10811v2)

[[17]](https://arxiv.org/html/2409.12538v1#:~:text=of%20utilizing%20LLM,reliance%20and) Liu, Y., Sharma, P., Oswal, M. J., Xia, H., & Huang, Y. (2025). PersonaFlow: Designing LLM-Simulated Expert Perspectives for Enhanced Research Ideation. Proceedings of the 2025 ACM Designing Interactive Systems Conference, 506–534. [https://arxiv.org/html/2409.12538v1](https://arxiv.org/html/2409.12538v1)

[[18]](https://joongishin.github.io/perGenWorkflow/material/persona-generation-workflow.pdf#:~:text=bring%20to%20the%20generation%20of,We%20found) Understanding Human–AI Workflows for Generating Personas [https://joongishin.github.io/perGenWorkflow/material/persona-generation-workflow.pdf](https://joongishin.github.io/perGenWorkflow/material/persona-generation-workflow.pdf)

[[19]](https://arxiv.org/html/2312.11970v1#:~:text=First%2C%20the%20LLM%20agent%20can,three%20strengths%2C%20LLM%20agents%20have) Gao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., & Li, Y. (2023). Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives [https://arxiv.org/html/2312.11970v1](https://arxiv.org/html/2312.11970v1)

[[20]](https://proceedings.neurips.cc/paper_files/paper/2024/file/1cb57fcf7ff3f6d37eebae5becc9ea6d-Paper-Conference.pdf#:~:text=have%20our%20second%20core%20finding%3A,human%20interactions%20and%20societal%20institutions) Xie, C., Chen, C., Jia, F., Ye, Z., Lai, S., Shu, K., Gu, J., Bibi, A., Hu, Z., Jurgens, D., Evans, J., Torr, P. H. S., Ghanem, B., & Li, G. (2024). Can Large Language Model Agents Simulate Human Trust Behavior? In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, & C. Zhang (Eds.), Advances in Neural Information Processing Systems 37 (NeurIPS 2024). NeurIPS. [https://proceedings.neurips.cc/paper_files/paper/2024/file/1cb57fcf7ff3f6d37eebae5becc9ea6d-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2024/file/1cb57fcf7ff3f6d37eebae5becc9ea6d-Paper-Conference.pdf)
